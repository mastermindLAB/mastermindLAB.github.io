---
layout: archive
permalink: /resume_page/
title: "Suranjit Banik"
author_profile: true
excerpt: "Focusing on Data Driven Business Analysis"
header:
  overlay_image: /images/creative-idea.jpg
  overlay_filter: rgba(0, 19, 26, 0.5)
  caption: "suranjitbanik"
  actions:
    - label: "See Projects"
      url: "https://github.com/mastermindlab"
---
<a href="/Users/suranjitbanik/gihubprotfolio/mastermindLAB.github.io/_pages/Suranjit_Resume_dec_2023.pdf" download="Suranjit_Resume_DataEngineer.pdf" class="download-button">Download Resume</a>


:iphone: (647)-616-9494

## Data Engineer

Data Engineer with 3+ years of experience in building data intensive applications, tackling challenging architectural
and scalability problems in Media. Currently helping groupM with Petabyte scale data pipelines.


### SKILLS

>Google Cloud Platform (GCP) • Database Design • Python (Programming Language) • BigQuery • Databricks


### EXPERIENCE

> Data Engineer

GroupM Nexus· Permanent Full-time <br>
Aug 2021 - Present · 2 yrs 5 mos Aug 2021 - Present <br>
Toronto, Ontario, Canada · HybridToronto, Ontario, Canada · Hybrid <br>

Databricks - PySpark, Sql (Hive) on GCP
Building Data Pipeline infrastructure on Google Cloud Platform, Azure and AWS<br>
Skills: Python, SQL, PySpark, BigQuery, Airflow, GitHub

>Achievements/Tasks

* Built streaming services to process real-time data for users, enabling timely insights and enhanced user
experience.
* Automated the API integration for campaign management programmatically in for Amazon Ads partner and The Trade Desk Rest APIs.
Optimized ETL processes and SQL queries, resulting in a 30% reduction in data processing time and improved overall system efficiency.
Successfully led a migration of a legacy Data Warehouse to GCP, resulting in enhanced scalability and improved data processing capabilities.
Developed infrastructure to process 15 TB of data per day, leading to an 8% increase in online sales for the Ad-tech division.
Implemented data quality checks, resulting in a 95% improvement in data accuracy and reliability.
* Working on scripting and API to send apart to the stakeholders for daily business development.Databricks - PySpark, Sql (Hive) on GCP Building Data Pipeline infrastructure on Google Cloud Platform, Azure and AWS Skills: Python, SQL, PySpark, BigQuery, Airflow, GitHub Built streaming services to process real-time data for users, enabling timely insights and enhanced user experience. Automated the API integration for campaign management programmatically in for Amazon Ads partner and The Trade Desk Rest APIs. Optimized ETL processes and SQL queries, resulting in a 30% reduction in data processing time and improved overall system efficiency.

> Data Engineer

Clue 1 yr 6 mos <br>
Permanent Full-time <br>
Jan 2021 - Jul 2021 · 7 mon <br>
Toronto, Ontario, Canada · HybridToronto, Ontario, Canada · Hybrid<br>

>Achievements/Tasks

* Worked on Graph Data Models.
Process: ETL (Apache HOP for data ingestion) + Data Modeling (Cypher Queries) + ML (Link Prediction Model Neo4j plugins) + Uses of AWS Services (S3, Athena, SageMaker, DataBrew). Implementing a GraphDB (one place to answer all questions for digital marketers [for better future])Worked on Graph Data Models. Process: ETL (Apache HOP for data ingestion) + Data Modeling (Cypher Queries) + ML (Link Prediction Model Neo4j plugins) + Uses of AWS Services (S3, Athena, SageMaker, DataBrew). Implementing a GraphDB (one place to answer all questions for digital marketers.

* Skills: Oral Communication · Digital Media · Communication · Microsoft Azure · Amazon Web Services (AWS) · Database Design · Data Modeling · Analytical Skills · Business Analytics · Reporting and Analysis · Data Warehousing · Relational Databases · Datorama · Problem Solving · Data Analytics · SQL Azure · Graph Databases · Neo4jSkills: Oral Communication · Digital Media · Communication · Microsoft Azure · Amazon Web Services (AWS) · Database Design · Data Modeling · Analytical Skills · Business Analytics · Reporting and Analysis · Data Warehousing · Relational Databases · Datorama · Problem Solving · Data Analytics · SQL Azure · Graph Databases · Neo4j

> Data Analyst
Clue <br>
Permanent Full-time <br>
Feb 2020 - Dec 2020 · 11 mon <br>
Toronto, Canada AreaToronto, Canada Area <br>

>Achievements/Tasks

* Worked on building Microsoft Azure's new analytics service to analyze media ads data and salesforce data ingestion for higher accuracy of parameters that matters before any campaign lunches.

* This capability of data integration includes: 

* Securing your data by making Azure blob storage and push it to the database for any advanced analytics.

* Skills: Oral Communication · Digital Media · CommunicationSkills: Oral Communication · Digital Media · Communication

> Digital Analyst
Mindshare<br>
May 2019 - Feb 2020 · 10 mon<br>
Toronto, Canada AreaToronto, Canada Area<br>

>Achievements/Tasks


* Worked on AI-powered marketing intelligence which can make smarter decisions by connecting and acting on all of the marketing data, investments, and KPIs. My current project is one of the world's Biggest CPG clients Nestle, building the entire Architecture on Datorama from scratch.

Integration

* Ad verification: MOAT
Programmatic: DV360, Amazon advertising 
Social: Facebook, Twitter, Spanchat, Pinterest, LinkedIn
Search: Amazon, Google Ads, Bing Ads, Criteo, Search 360, Pacvue
Analytics: Google Analytics

* Audience Planning: Salesforce Audience Studio (Krux), GA360Worked on AI-powered marketing intelligence which can make smarter decisions by connecting and acting on all of the marketing data, investments, and KPIs. My current project is one of the world's Biggest CPG clients Nestle, building the entire Architecture on Datorama from scratch. Integration Ad verification: MOAT Programmatic: DV360, Amazon advertising Social: Facebook, Twitter, Spanchat, Pinterest, LinkedIn Search: Amazon, Google Ads, Bing Ads, Criteo, Search 360, Pacvue Analytics: Google Analytics Audience Planning: Salesforce Audience Studio (Krux), GA360

* Skills: Oral Communication · Digital Media · Communication

>Programmer

Sysnova Information System Limited<br>
03/2013 – 08/2016

>Achievements/Tasks

* Designed and implemented key measurement reports
for conveying metrics to customers, monitoring service
levels and attainments, identifying trends and
performing root cause analysis.

* Prepared forecasts and identiﬁed trends through data
analysis and tracking resulting 90% improvement in the
marketing strategy.

* Developed SQL queries to obtain complex data from
tables in remote databases.
Awarded from the Sales department for 40% more
eﬀective analysis to ﬁnd out which product has a huge
impact on the current market.

* Conducted ﬁeld studies and data collection to support
sophisticated analysis, managing multiple projects.
Identiﬁed business requirements and devised
implementation strategies to solve business problems.

### EDUCATION

>Electrical & Computer Engineering <br>
 University of Ottawa, Canada<br>
09/2016 – 01/2018

>Bachelor in Computer Science and Engineering<br>
Visvesvaraya Technological University, India<br>
09/2008 – 04/2013

### PROJECTS

* Predicting the Survival rate of Titanic Passengers<br>
_(02/2019 – 03/2019)_

* Used numpy, pandas and seaborn lib to visualize and analyse
titanic's passengers survival rate.

* Finding risk for future investment in share market of
top Trending Tickers i.e. Google, Amazon, Tesla,
Microsoft, and Apple. Forecasting and market analysis.<br>
_(03/2019 – 03/2019)_

* Data wrangling, creaning, forecasting, predictive analysis and
visualization.
Advanced use of Python pandas,seaborn library. Value at risk using
'Bootstrap' method.
Calculating Value at risk using 'Monte Carlo' method.
Scatter plot comparison of each share market's price.

* [see more projects](https://www.github.com/mastermindlab/)

### Publications

* A paper accepted for the 2018 IEEE Canadian Conference
on Electrical & Computer Engineering (CCECE) Communications and Networks “A Review of Recently
Emerging Denial of Service Threats and Defenses in the
Transport Layer” paper information: # 1570428369.

* Smart-city in IOT, Used a machine-learning algorithm to
implement an objective function of ﬁnding the noise in
the channel using python.

### CERTIFICATE
* Building Batch Data Pipelines on Google CloudBuilding Batch Data Pipelines on Google Cloud
Google
Issued Dec 2023 Credential ID 6690524
* Academy Accreditation - Generative AI FundamentalsAcademy Accreditation - Generative AI Fundamentals
Databricks
Issued Aug 2023 · Expires Aug 2025Issued Aug 2023 · Expires Aug 2025
Credential ID 79304079
Skills: Artificial Intelligence
* Modernizing Data Lakes and Data Warehouses with Google CloudModernizing Data Lakes and Data Warehouses with Google Cloud
Google
Issued Aug 2023
Credential ID 4676956
Skills: BigQuery

### INTERESTS
`Toronto Data Science & Big Data Meetup` `Photography`

`Painting` `Machine learning` `CODING` `Reading`

`Data Engineering` `Travelling` `Ice Skating`
